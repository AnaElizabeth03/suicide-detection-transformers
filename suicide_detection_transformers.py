# -*- coding: utf-8 -*-
"""Modelos Preentrenados.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rVwsWP5MoycwK1E9WYj3WVI23rBu9DQ8
"""

!pip install transformers

!pip install numpy==1.26.4 --force-reinstall --no-cache-dir

import pandas as pd
from sklearn.model_selection import train_test_split
from datasets import Dataset

# Cargar CSV
csv_file = "/content/drive/MyDrive/IA-NLP/Suicide_Detection.csv"
df = pd.read_csv(csv_file)

# Limpiar y eliminar nulos
df = df.dropna()

# Verificar las clases y balancear (5000 + 5000)
df_positive = df[df['class'] == 'suicide'].sample(5000, random_state=42)
df_negative = df[df['class'] == 'non-suicide'].sample(5000, random_state=42)
df_small = pd.concat([df_positive, df_negative]).sample(frac=1, random_state=42).reset_index(drop=True)

# Codificar las clases como 0 y 1
df_small['label'] = df_small['class'].map({'non-suicide': 0, 'suicide': 1})

# Dividir en train/test
train_df, test_df = train_test_split(df_small, test_size=0.2, stratify=df_small['label'], random_state=42)

# Convertir a HuggingFace Datasets
train_dataset_raw = Dataset.from_pandas(train_df[['text', 'label']])
test_dataset_raw = Dataset.from_pandas(test_df[['text', 'label']])

#Tokenizar para DISTILBEART
from transformers import DistilBertTokenizerFast

tokenizer_distilbert = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')

def tokenize_distilbert(examples):
    return tokenizer_distilbert(examples['text'], truncation=True, padding=True, max_length=128)

train_distilbert = train_dataset_raw.map(tokenize_distilbert, batched=True)
test_distilbert = test_dataset_raw.map(tokenize_distilbert, batched=True)

train_distilbert = train_distilbert.rename_column("label", "labels")
test_distilbert = test_distilbert.rename_column("label", "labels")
train_distilbert.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])
test_distilbert.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])

#Tokenizar para BERT
from transformers import BertTokenizerFast

tokenizer_bert = BertTokenizerFast.from_pretrained('bert-base-uncased')

def tokenize_bert(examples):
    return tokenizer_bert(examples['text'], truncation=True, padding=True, max_length=128)

train_bert = train_dataset_raw.map(tokenize_bert, batched=True)
test_bert = test_dataset_raw.map(tokenize_bert, batched=True)

train_bert = train_bert.rename_column("label", "labels")
test_bert = test_bert.rename_column("label", "labels")
train_bert.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])
test_bert.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])

#Tokenizar para RoBERTa
from transformers import RobertaTokenizerFast

tokenizer_roberta = RobertaTokenizerFast.from_pretrained('roberta-base')

def tokenize_roberta(examples):
    return tokenizer_roberta(examples['text'], truncation=True, padding=True, max_length=128)

train_roberta = train_dataset_raw.map(tokenize_roberta, batched=True)
test_roberta = test_dataset_raw.map(tokenize_roberta, batched=True)

train_roberta = train_roberta.rename_column("label", "labels")
test_roberta = test_roberta.rename_column("label", "labels")
train_roberta.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])
test_roberta.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])

#Definir metricas
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = logits.argmax(axis=-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')
    acc = accuracy_score(labels, preds)
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }

#Entrenamiento de DISTILBEART
from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments

model_distilbert = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)

training_args_distilbert = TrainingArguments(
    output_dir='./results_distilbert',
    eval_strategy="epoch",
    save_strategy="epoch",
    num_train_epochs=3,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    learning_rate=2e-5,
    weight_decay=0.01,
    load_best_model_at_end=True,
    metric_for_best_model="f1",
    logging_dir='./logs_distilbert',
    report_to="none"
)

trainer_distilbert = Trainer(
    model=model_distilbert,
    args=training_args_distilbert,
    train_dataset=train_distilbert,
    eval_dataset=test_distilbert,
    compute_metrics=compute_metrics
)

trainer_distilbert.train()
results_distilbert = trainer_distilbert.evaluate()
print("DistilBERT:", results_distilbert)

#Entrenamiento de BERT
from transformers import BertForSequenceClassification

model_bert = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)

training_args_bert = TrainingArguments(
    output_dir='./results_bert',
    eval_strategy="epoch",
    save_strategy="epoch",
    num_train_epochs=3,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    learning_rate=2e-5,
    weight_decay=0.01,
    load_best_model_at_end=True,
    metric_for_best_model="f1",
    logging_dir='./logs_bert',
    report_to="none"
)

trainer_bert = Trainer(
    model=model_bert,
    args=training_args_bert,
    train_dataset=train_bert,
    eval_dataset=test_bert,
    compute_metrics=compute_metrics
)

trainer_bert.train()
results_bert = trainer_bert.evaluate()
print("BERT:", results_bert)

#Entrenamiento de RoBERTa
from transformers import RobertaForSequenceClassification

model_roberta = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)

training_args_roberta = TrainingArguments(
    output_dir='./results_roberta',
    eval_strategy="epoch",
    save_strategy="epoch",
    num_train_epochs=3,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    learning_rate=2e-5,
    weight_decay=0.01,
    load_best_model_at_end=True,
    metric_for_best_model="f1",
    logging_dir='./logs_roberta',
    report_to="none"
)

trainer_roberta = Trainer(
    model=model_roberta,
    args=training_args_roberta,
    train_dataset=train_roberta,
    eval_dataset=test_roberta,
    compute_metrics=compute_metrics
)

trainer_roberta.train()
results_roberta = trainer_roberta.evaluate()
print("RoBERTa:", results_roberta)

#Matriz de DISTILBEART
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Obtener predicciones
predictions_distilbert = trainer_distilbert.predict(test_distilbert)
y_true_distilbert = predictions_distilbert.label_ids
y_pred_distilbert = predictions_distilbert.predictions.argmax(axis=1)

# Graficar matriz de confusión
cm = confusion_matrix(y_true_distilbert, y_pred_distilbert)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["non-suicide", "suicide"])
disp.plot(cmap='Blues')
plt.title("DistilBERT - Matriz de Confusión")
plt.show()

#Matriz BERT
predictions_bert = trainer_bert.predict(test_bert)
y_true_bert = predictions_bert.label_ids
y_pred_bert = predictions_bert.predictions.argmax(axis=1)

# Graficar matriz de confusión
cm = confusion_matrix(y_true_bert, y_pred_bert)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["non-suicide", "suicide"])
disp.plot(cmap='Blues')
plt.title("BERT - Matriz de Confusión")
plt.show()

#Matriz RoBERTa
predictions_roberta = trainer_roberta.predict(test_roberta)
y_true_roberta = predictions_roberta.label_ids
y_pred_roberta = predictions_roberta.predictions.argmax(axis=1)

# Graficar matriz de confusión
cm = confusion_matrix(y_true_roberta, y_pred_roberta)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["non-suicide", "suicide"])
disp.plot(cmap='Blues')
plt.title("RoBERTa - Matriz de Confusión")
plt.show()

import matplotlib.pyplot as plt

def plot_training_metrics_v2(log_history, model_name):
    train_epochs = []
    train_loss = []
    eval_epochs = []
    eval_loss = []
    eval_accuracy = []

    for log in log_history:
        # Train loss logs
        if 'loss' in log and 'epoch' in log and 'eval_loss' not in log:
            train_epochs.append(log['epoch'])
            train_loss.append(log['loss'])
        # Eval logs
        if 'eval_loss' in log and 'epoch' in log:
            eval_epochs.append(log['epoch'])
            eval_loss.append(log['eval_loss'])
            eval_accuracy.append(log['eval_accuracy'])

    plt.figure(figsize=(12,5))

    # Loss plot
    plt.subplot(1,2,1)
    if train_epochs and train_loss:
        plt.plot(train_epochs, train_loss, label='Train Loss', marker='o')
    if eval_epochs and eval_loss:
        plt.plot(eval_epochs, eval_loss, label='Eval Loss', marker='o')
    plt.title(f'Loss por época - {model_name}')
    plt.xlabel('Época')
    plt.ylabel('Loss')
    plt.legend()

    # Accuracy plot
    plt.subplot(1,2,2)
    if eval_epochs and eval_accuracy:
        plt.plot(eval_epochs, eval_accuracy, label='Eval Accuracy', marker='o', color='green')
    plt.title(f'Accuracy por época - {model_name}')
    plt.xlabel('Época')
    plt.ylabel('Accuracy')
    plt.legend()

    plt.show()

plot_training_metrics_v2(trainer_distilbert.state.log_history, 'DistilBERT')
plot_training_metrics_v2(trainer_bert.state.log_history, 'BERT')
plot_training_metrics_v2(trainer_roberta.state.log_history, 'RoBERTa')

#comparacion
def extract_metrics(log_history):
    epochs = []
    train_loss = []
    eval_loss = []
    eval_accuracy = []

    for log in log_history:
        if 'loss' in log and 'epoch' in log and 'eval_loss' not in log:
            epochs.append(log['epoch'])
            train_loss.append(log['loss'])
        if 'eval_loss' in log and 'epoch' in log:
            # eval_loss y eval_accuracy se guardan en el mismo log
            eval_loss.append(log['eval_loss'])
            eval_accuracy.append(log['eval_accuracy'])

    return epochs, train_loss, eval_loss, eval_accuracy

import matplotlib.pyplot as plt

def plot_comparison(trainers_logs, model_names):
    plt.figure(figsize=(12,5))

    # Loss (Eval) comparativo
    plt.subplot(1,2,1)
    for log, name in zip(trainers_logs, model_names):
        _, _, eval_loss, _ = extract_metrics(log)
        epochs = list(range(1, len(eval_loss)+1))
        plt.plot(epochs, eval_loss, marker='o', label=f'{name} Eval Loss')
    plt.title('Comparación Eval Loss')
    plt.xlabel('Época')
    plt.ylabel('Loss')
    plt.legend()

    # Accuracy comparativo
    plt.subplot(1,2,2)
    for log, name in zip(trainers_logs, model_names):
        _, _, _, eval_acc = extract_metrics(log)
        epochs = list(range(1, len(eval_acc)+1))
        plt.plot(epochs, eval_acc, marker='o', label=f'{name} Eval Accuracy')
    plt.title('Comparación Eval Accuracy')
    plt.xlabel('Época')
    plt.ylabel('Accuracy')
    plt.legend()

    plt.show()

trainers_logs = [
    trainer_distilbert.state.log_history,
    trainer_bert.state.log_history,
    trainer_roberta.state.log_history
]

model_names = ['DistilBERT', 'BERT', 'RoBERTa']

plot_comparison(trainers_logs, model_names)

import torch
import torch.nn.functional as F

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Mueve los modelos al device adecuado
model_distilbert.to(device)
model_bert.to(device)
model_roberta.to(device)

def predict(text, model, tokenizer):
    model.eval()
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=128)
    # Mueve los inputs al mismo device del modelo
    inputs = {k: v.to(device) for k, v in inputs.items()}

    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
        probs = F.softmax(logits, dim=1)
        conf, pred = torch.max(probs, dim=1)
    etiqueta = 'suicide' if pred.item() == 1 else 'non-suicide'
    return etiqueta, conf.item()

# Ejemplo de textos
ejemplos = [
    "I am feeling happy and excited!",
    "I feel hopeless and want to give up.",
    "My future looks bright.",
    "I don't want to live anymore."
]

print("DistilBERT Predictions:")
for texto in ejemplos:
    etiqueta, conf = predict(texto, model_distilbert, tokenizer_distilbert)
    print(f"Texto: {texto}")
    print(f"Predicción: {etiqueta} (confianza: {conf:.2f})\n")

print("BERT Predictions:")
for texto in ejemplos:
    etiqueta, conf = predict(texto, model_bert, tokenizer_bert)
    print(f"Texto: {texto}")
    print(f"Predicción: {etiqueta} (confianza: {conf:.2f})\n")

print("RoBERTa Predictions:")
for texto in ejemplos:
    etiqueta, conf = predict(texto, model_roberta, tokenizer_roberta)
    print(f"Texto: {texto}")
    print(f"Predicción: {etiqueta} (confianza: {conf:.2f})\n")

#Guardar
base_path = "/content/drive/MyDrive/IA-NLP/Modelo_Entrenado"
import os
os.makedirs(base_path, exist_ok=True)

# Guardar DistilBERT
distilbert_path = os.path.join(base_path, "DistilBERT")
os.makedirs(distilbert_path, exist_ok=True)
model_distilbert.save_pretrained(distilbert_path)
tokenizer_distilbert.save_pretrained(distilbert_path)

# Guardar BERT
bert_path = os.path.join(base_path, "BERT")
os.makedirs(bert_path, exist_ok=True)
model_bert.save_pretrained(bert_path)
tokenizer_bert.save_pretrained(bert_path)

# Guardar RoBERTa
roberta_path = os.path.join(base_path, "RoBERTa")
os.makedirs(roberta_path, exist_ok=True)
model_roberta.save_pretrained(roberta_path)
tokenizer_roberta.save_pretrained(roberta_path)

print("Modelos y tokenizadores guardados exitosamente.")