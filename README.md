# Suicide Detection Using Transformers

This project explores the use of transformer-based models (BERT, DistilBERT, and RoBERTa) for detecting suicidal ideation in text data using NLP techniques.

## 📁 Repository Structure

- `transformers_suicide_detection.ipynb`: Main Colab notebook containing the full pipeline (data preprocessing, training, evaluation, and results).
- `transformers_suicide_detection.py`: Exported version of the notebook as a Python script (optional).
- `data/`: Folder for data (not uploaded if sensitive or large).
- `results/`: Output metrics, plots, and model results.

## 🧠 Models Used

- `bert-base-uncased`
- `distilbert-base-uncased`
- `roberta-base`

## 📊 Key Features

- Balanced dataset of suicide vs. non-suicide texts.
- Fine-tuning of pre-trained transformer models.
- Evaluation using accuracy, F1-score, confusion matrix, and more.
- Visualizations of model performance.
